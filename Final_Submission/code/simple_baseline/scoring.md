For the evaluation script, we chose to implement Rouge-N. Rouge stands for "Recall-Oriented Understudy for Gisting Evaluation". Although we have implemented our ROUGE program to generate evaluations with any number of n-grams, we look at bigrams. The bigram measure gives a better idea of the generated summary's goodness in terms of not just words, but also word order (and thus grammaticallity, correct context-meaning, and comprehensibility).

In its most basic form, Rouge takes two summaries (a system generated summary and a gold summary), and computes the following:

Recall = (number of overlapping words)/(total words in gold summary)
Precision = (number of overlapping words)/(total words in generated summary)
and F-score from the harmonic mean of recall and precision

For Rouge-N, the formulas above can be adapted to be:
Recall = (number of overlapping n-grams)/(total n-grams in gold summary)
Precision = (number of overlapping n-grams)/(total n-grams in generated summary)
and F-score from the harmonic mean of recall and precision

The metrics for precision and recall can be obtained by running score.py.

ROUGE was first introduced for text summarization in ACL in 2003.
See paper by Chin-Yew Lin (http://www.aclweb.org/anthology/W04-1013). There it is stated that ROUGE-2, ROUGE-L and ROUGE-W worked well in single document summarization tasks, which validates our choice of ROUGE-2 as the selected evaluation method.
Wikipedia article about ROUGE: https://en.wikipedia.org/wiki/ROUGE_(metric)

Since the measures are precision, recall, and f-score, higher scores are better.

score.py takes in a goldfile (a list of just gold summaries), a predfile of the summaries generated by the summarizer (simple-baseline.py), and a positive integer n, the size of the n-grams to check.

ex: python3 score.py --goldfile sumdata/bothtestsumonly.txt --predfile base_test_pred.txt --ngram 2

This outputs:

Recall: 0.03750891711392287
Precision: 0.07162823896253027
FScore: 0.04923525176667997
